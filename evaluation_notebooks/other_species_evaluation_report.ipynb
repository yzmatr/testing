{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7909b2c5-934c-48eb-a01f-5c8fec15df00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üê∏ Comprehensive BirdNET Analysis\n",
    "\n",
    "This notebook provides a comprehensive analysis of the BirdNET-based frog species classification experiment results from MLflow run ID: `28b89d862b80460bad606ffb57913680`.\n",
    "\n",
    "## üéØ Overview\n",
    "- **Experiment**: BirdNET 48kHz averaging multiclass balanced with \"Other\" species\n",
    "- **Overall Accuracy**: 82.63%\n",
    "- **Target Species**: 15 frog species + \"Other\" class\n",
    "- **Feature Extraction**: BirdNET embeddings (1024-dim) with element-wise averaging\n",
    "- **Test Dataset**: 2,700 samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mlflow\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML, Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"üì¶ Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mlflow\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML, Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"üì¶ Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get run ID from environment variable (set by training script) or change the default \n",
    "import os\n",
    "TARGET_RUN_ID = os.getenv('MLFLOW_RUN_ID', \"28b89d862b80460bad606ffb57913680\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup MLflow connection with Databricks environment detection\n",
    "# Detect if running on Databricks\n",
    "IS_DATABRICKS = \"DATABRICKS_RUNTIME_VERSION\" in os.environ\n",
    "\n",
    "if IS_DATABRICKS:\n",
    "    # On Databricks, MLflow is automatically configured\n",
    "    print(\"üîó Running on Databricks - Using Databricks MLflow tracking\")\n",
    "    ROOT_DIR = Path.cwd().parent  # Adjust for bundle structure\n",
    "else:\n",
    "    # Local development setup\n",
    "    ROOT_DIR = Path(os.getcwd()).parent.parent  # notebook -> project root\n",
    "    MLFLOW_TRACKING_URI = os.getenv('MLFLOW_TRACKING_URI', f\"sqlite:///{ROOT_DIR}/mlops/mlflow/tracking.db\")\n",
    "    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "    print(f\"üîó Local development - MLflow URI: {mlflow.get_tracking_uri()}\")\n",
    "\n",
    "print(f\"üìÅ Root DIR: {ROOT_DIR}\")\n",
    "print(f\"üîó MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"üéØ Target run ID: {TARGET_RUN_ID}\")\n",
    "print(f\"üîÑ Environment: {'Databricks' if IS_DATABRICKS else 'Local'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MLflow run data - Handle missing metadata case\n",
    "\n",
    "run = mlflow.get_run(TARGET_RUN_ID)\n",
    "experiment = mlflow.get_experiment(run.info.experiment_id)\n",
    "\n",
    "# Extract data from run\n",
    "params = run.data.params\n",
    "metrics = run.data.metrics\n",
    "artifact_path = Path(run.info.artifact_uri.replace(\"file://\", \"\"))\n",
    "\n",
    "print(f\"‚úÖ Loaded run: {run.info.run_id}\")\n",
    "print(f\"‚úÖ Experiment: {experiment.name}\")\n",
    "print(f\"üìÖ Start time: {pd.to_datetime(run.info.start_time, unit='ms')}\")\n",
    "print(f\"üìà Status: {run.info.status}\")\n",
    "print(f\"üìÅ Artifact path: {artifact_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9cbe85e-c30a-48ad-9633-9f78919bf96d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚öôÔ∏è Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display key experiment parameters - Load from config if params not available\n",
    "if not params:\n",
    "    print(\"‚ÑπÔ∏è Loading configuration from artifacts...\")\n",
    "    # Try to load from config files\n",
    "    config_path = artifact_path / \"config\"\n",
    "    if config_path.exists():\n",
    "        try:\n",
    "            # Load from experiment config if available\n",
    "            exp_config_path = config_path / \"experiment_config.txt\"\n",
    "            if exp_config_path.exists():\n",
    "                with open(exp_config_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        if '=' in line and not line.startswith('#'):\n",
    "                            key, value = line.split('=', 1)\n",
    "                            params[key.strip()] = value.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ÑπÔ∏è Could not load config file: {e}\")\n",
    "\n",
    "# Load configuration from actual config files\n",
    "config_values = {}\n",
    "try:\n",
    "    import json\n",
    "    \n",
    "    # Load data selector config\n",
    "    data_selector_config_path = artifact_path / \"config\" / \"data_selector_config.json\"\n",
    "    if data_selector_config_path.exists():\n",
    "        with open(data_selector_config_path, 'r') as f:\n",
    "            data_selector_config = json.load(f)\n",
    "            config_values.update({\n",
    "                'max_samples_per_class': data_selector_config.get('max_samples_per_class', 1000),\n",
    "                'other_boost_factor': data_selector_config.get('other_species_boost_factor', 3.0),\n",
    "                'num_target_species': len(data_selector_config.get('target_species', [])),\n",
    "                'sampling_strategy': data_selector_config.get('sampling_strategy', 'downsample')\n",
    "            })\n",
    "    \n",
    "    # Load preprocessor config\n",
    "    preprocessor_config_path = artifact_path / \"config\" / \"preprocessor_config.json\"\n",
    "    if preprocessor_config_path.exists():\n",
    "        with open(preprocessor_config_path, 'r') as f:\n",
    "            preprocessor_config = json.load(f)\n",
    "            # Parse the chunking config string to extract values\n",
    "            chunking_str = preprocessor_config.get('chunking', '')\n",
    "            if 'sr=48000' in chunking_str:\n",
    "                config_values['audio_sr'] = 48000\n",
    "            if 'window_duration=30' in chunking_str:\n",
    "                config_values['window_duration'] = 30\n",
    "            \n",
    "            # Parse BirdNET config\n",
    "            birdnet_str = preprocessor_config.get('birdnet_config', '')\n",
    "            if 'segment_duration=3.0' in birdnet_str:\n",
    "                config_values['segment_duration'] = 3.0\n",
    "                \n",
    "            config_values['feature_type'] = 'BirdNET embeddings'\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è Could not load all config files: {e}\")\n",
    "\n",
    "# Create key parameters with correct values from config files\n",
    "key_params = {\n",
    "    'Number of Target Species': str(config_values.get('num_target_species', params.get('num_target_species', '15'))),\n",
    "    'Total Classes': str(config_values.get('num_target_species', 15) + 1),  # target species + Other\n",
    "    'Max Samples per Class': str(config_values.get('max_samples_per_class', params.get('max_samples_per_class', '1000'))),\n",
    "    'Audio Sample Rate': f\"{config_values.get('audio_sr', params.get('audio_sr', '48000'))} Hz\",\n",
    "    'Window Duration': f\"{config_values.get('window_duration', params.get('window_duration', '30'))} seconds\",\n",
    "    'BirdNET Segment Duration': f\"{config_values.get('segment_duration', params.get('segment_duration', '3'))} seconds\",\n",
    "    'Feature Type': config_values.get('feature_type', params.get('feature_type', 'BirdNET embeddings')),\n",
    "    'Model Architecture': params.get('model_architecture', 'Dense layers'),\n",
    "    'Hidden Layers': params.get('hidden_layers', '[256, 128]'),\n",
    "    'Dropout Rate': params.get('dropout_rate', '0.3'),\n",
    "    'Batch Size': params.get('batch_size', '32'),\n",
    "    'Learning Rate': params.get('learning_rate', '0.001'),\n",
    "    'Epochs': params.get('epochs', '100'),\n",
    "    'Other Species Boost Factor': f\"{config_values.get('other_boost_factor', params.get('other_boost_factor', '3.0'))}x\"\n",
    "}\n",
    "\n",
    "params_df = pd.DataFrame(list(key_params.items()), columns=['Parameter', 'Value'])\n",
    "display(params_df.style.set_properties(**{'text-align': 'left'}).hide(axis='index'))\n",
    "\n",
    "# Store key values for later use\n",
    "try:\n",
    "    num_classes = int(params.get('total_num_classes', 16))\n",
    "    num_target_species = int(params.get('num_target_species', 15))\n",
    "except (ValueError, TypeError):\n",
    "    num_classes = 16\n",
    "    num_target_species = 15\n",
    "\n",
    "print(f\"\\nüéØ Configuration loaded: {len(key_params)} parameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41814a22-d233-49ed-8e3d-8123e0583cd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìä Per-Species Classification Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load classification report and species mapping from MLflow artifacts\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "\n",
    "# Load from artifacts\n",
    "# Download both files from MLflow artifacts\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    # Load classification report\n",
    "    classification_report_path = mlflow.artifacts.download_artifacts(\n",
    "        run_id=TARGET_RUN_ID,\n",
    "        artifact_path=\"data_evaluation/classification_report.parquet\",\n",
    "        dst_path=temp_dir\n",
    "    )\n",
    "    classification_report_df = pd.read_parquet(classification_report_path)\n",
    "    \n",
    "    # Load species mapping\n",
    "    species_mapping_path = mlflow.artifacts.download_artifacts(\n",
    "        run_id=TARGET_RUN_ID,\n",
    "        artifact_path=\"data_evaluation/species_id_class_mapping.parquet\",\n",
    "        dst_path=temp_dir\n",
    "    )\n",
    "    species_mapping_df = pd.read_parquet(species_mapping_path)\n",
    "\n",
    "# Create mapping from class_id to species_name and species_id\n",
    "class_to_species = dict(zip(species_mapping_df['class_id'], species_mapping_df['species_name']))\n",
    "class_to_species_id = dict(zip(species_mapping_df['class_id'], species_mapping_df.get('species_id', species_mapping_df['class_id'])))\n",
    "print(f\"‚úÖ Loaded species mapping for {len(class_to_species)} classes\")\n",
    "\n",
    "# Convert numeric indices to species names with IDs or summary labels\n",
    "def map_class_to_species_with_id(idx):\n",
    "    if isinstance(idx, (int, str)) and str(idx).isdigit():\n",
    "        class_id = int(idx)\n",
    "        # Handle summary statistics indices that appear at the end\n",
    "        if class_id == 16:\n",
    "            return 'accuracy'\n",
    "        elif class_id == 17:\n",
    "            return 'macro avg'\n",
    "        elif class_id == 18:\n",
    "            return 'weighted avg'\n",
    "        else:\n",
    "            species_name = class_to_species.get(class_id, f'Class_{class_id}')\n",
    "            species_id = class_to_species_id.get(class_id, class_id)\n",
    "            return f\"{species_name} (ID: {species_id})\"\n",
    "    return str(idx)\n",
    "\n",
    "# Apply mapping and filter individual classes\n",
    "classification_report_df.index = [map_class_to_species_with_id(idx) for idx in classification_report_df.index]\n",
    "classification_report_df.index.name = 'Species (ID)'\n",
    "\n",
    "# Separate individual classes from summary statistics\n",
    "summary_rows = classification_report_df[classification_report_df.index.isin(['accuracy', 'macro avg', 'weighted avg'])]\n",
    "individual_classes = classification_report_df[\n",
    "    ~classification_report_df.index.isin(['accuracy', 'macro avg', 'weighted avg'])\n",
    "].sort_values('f1-score', ascending=False)\n",
    "\n",
    "print(f\"üìä CLASSIFICATION REPORT - Individual Species Performance (Ordered by F1-Score)\")\n",
    "print(\"=\" * 90)\n",
    "display(individual_classes.round(3).style.background_gradient(\n",
    "    cmap='RdYlGn', subset=['precision', 'recall', 'f1-score']\n",
    ").format(precision=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5e9efa7-af50-42cd-b148-826fa7ba49f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üîç Enhanced \"Other\" Species Analysis\n",
    "\n",
    "This section provides detailed analysis of how the \"Other\" class performs, including which non-target species are most commonly confused with target species.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    # Load classification report\n",
    "    test_ids_path = mlflow.artifacts.download_artifacts(\n",
    "        run_id=TARGET_RUN_ID,\n",
    "        artifact_path=\"data_input/test_ids.parquet\",\n",
    "        dst_path=temp_dir\n",
    "    )\n",
    "    test_ids_df = pd.read_parquet(test_ids_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is named test_ids_df\n",
    "# Filter rows where label == 15\n",
    "label_15_df = test_ids_df[test_ids_df['label'] == 15]\n",
    "\n",
    "# Count occurrences of each species\n",
    "species_counts = label_15_df['species_name'].value_counts()\n",
    "\n",
    "# Display the full list\n",
    "print(\"Full list of species with label = 15 and their counts:\\n\")\n",
    "print(species_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of unique 'Other' species: {label_15_df['species_name'].nunique()}\")\n",
    "print(\"Top 10 'Other' species by count:\")\n",
    "print(species_counts.head(10))\n",
    "print(\"Species with only 1 sample:\", (species_counts == 1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_samples = species_counts[species_counts < 5]\n",
    "print(f\"Species with <5 samples: {len(few_samples)} ({len(few_samples)/len(species_counts)*100:.1f}%)\")\n",
    "\n",
    "top_5_sum = species_counts.head(5).sum()\n",
    "total = species_counts.sum()\n",
    "print(f\"Top 5 'Other' species account for {top_5_sum}/{total} ({top_5_sum/total*100:.1f}%) of 'Other' samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the row for label 15 (Other species)\n",
    "other_row = classification_report_df.loc[classification_report_df.index.str.contains(r'\\b15\\b|Other', case=False, regex=True)]\n",
    "\n",
    "print(\"=== Overall 'Other Species' Category Performance ===\")\n",
    "print(other_row[['precision', 'recall', 'f1-score', 'support']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample count per species\n",
    "print(\"Top 10 'Other' species by count:\")\n",
    "print(species_counts.head(10))\n",
    "\n",
    "# If you have more columns (e.g., duration, quality)\n",
    "if 'duration_seconds' in label_15_df.columns:\n",
    "    print(label_15_df.groupby('species_name')['duration_seconds'].describe().head(10))\n",
    "\n",
    "# Visualization\n",
    "species_counts.head(20).plot(kind='barh', figsize=(8,6), title=\"Top 20 'Other' Species by Count\")\n",
    "plt.xlabel(\"Sample Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    # Load classification report\n",
    "    error_analysis_path = mlflow.artifacts.download_artifacts(\n",
    "        run_id=TARGET_RUN_ID,\n",
    "        artifact_path=\"data_evaluation/test_predictions_error_analysis.parquet\",\n",
    "        dst_path=temp_dir\n",
    "    )\n",
    "    error_df = pd.read_parquet(error_analysis_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall performance for 'Other' (class 15)\n",
    "other_mask = error_df['actual_class'] == 15\n",
    "other_total = other_mask.sum()\n",
    "other_correct = (other_mask & error_df['is_correct']).sum()\n",
    "other_acc = other_correct / other_total\n",
    "print(f\"Other species accuracy: {other_acc:.3%} ({other_correct}/{other_total})\")\n",
    "\n",
    "# Error type breakdown for 'Other'\n",
    "if 'error_type' in error_df.columns:\n",
    "    print(\"Error type breakdown for 'Other':\")\n",
    "    display(error_df[other_mask]['error_type'].value_counts().to_frame('count'))\n",
    "    display(error_df[other_mask]['error_type'].value_counts(normalize=True).to_frame('proportion'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual species within 'Other'\n",
    "if 'validated_frog_names' in error_df.columns:\n",
    "    other_species_counts = error_df[other_mask]['validated_frog_names'].value_counts()\n",
    "    print(\"Top 10 'Other' species by count:\")\n",
    "    display(other_species_counts.head(10))\n",
    "    print(\"Species with only 1 sample:\", (other_species_counts == 1).sum())\n",
    "\n",
    "    # Per-species accuracy (if enough samples)\n",
    "    per_species = error_df[other_mask].groupby('validated_frog_names')['is_correct'].agg(['count', 'mean'])\n",
    "    per_species = per_species.rename(columns={'mean': 'accuracy'})\n",
    "    display(per_species.sort_values('count', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top confusion patterns: Other species misclassified as target species\n",
    "other_as_target = error_df[(error_df['actual_class'] == 15) & (error_df['pred_class'] != 15)]\n",
    "\n",
    "# Group by (Other species, Confused Target), count cases and compute mean confidence\n",
    "confusion_summary = (\n",
    "    other_as_target\n",
    "    .groupby(['validated_frog_names', 'pred_class'])\n",
    "    .agg(\n",
    "        cases=('id', 'count'),\n",
    "        mean_confidence=('pred_class_prob', 'mean')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Map pred_class to species name\n",
    "confusion_summary['confused_target'] = confusion_summary['pred_class'].map(class_to_species)\n",
    "# Total cases per 'Other' species for confusion rate\n",
    "total_per_other = other_as_target.groupby('validated_frog_names')['id'].count()\n",
    "confusion_summary['total_cases'] = confusion_summary['validated_frog_names'].map(total_per_other)\n",
    "confusion_summary['confusion_rate'] = confusion_summary['cases'] / confusion_summary['total_cases']\n",
    "confusion_summary['rate_cases'] = confusion_summary['cases'].astype(str) + ' / ' + confusion_summary['total_cases'].astype(str)\n",
    "\n",
    "# Sort by number of cases (descending)\n",
    "confusion_summary = confusion_summary.sort_values('cases', ascending=False)\n",
    "\n",
    "# Display top 10 confusion patterns\n",
    "display(\n",
    "    confusion_summary[['validated_frog_names', 'confused_target', 'confusion_rate', 'rate_cases', 'mean_confidence']]\n",
    "    .rename(columns={\n",
    "        'validated_frog_names': 'Other Species',\n",
    "        'confused_target': 'Confused Target',\n",
    "        'confusion_rate': 'Confusion Rate',\n",
    "        'rate_cases': 'Cases/All',\n",
    "        'mean_confidence': 'Confidence'\n",
    "    })\n",
    "    .head(10)\n",
    "    .style.format({'Confusion Rate': '{:.2%}', 'Confidence': '{:.3f}'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target species confused as 'Other'\n",
    "target_as_other = error_df[(error_df['actual_class'] != 15) & (error_df['pred_class'] == 15)]\n",
    "\n",
    "# Group by (Target species), count cases and compute mean confidence\n",
    "target_confusion = (\n",
    "    target_as_other\n",
    "    .groupby('actual_class')\n",
    "    .agg(\n",
    "        cases=('id', 'count'),\n",
    "        mean_confidence=('pred_class_prob', 'mean')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "target_confusion['target_species'] = target_confusion['actual_class'].map(class_to_species)\n",
    "# Total cases per target species for confusion rate\n",
    "total_per_target = error_df[error_df['actual_class'] != 15].groupby('actual_class')['id'].count()\n",
    "target_confusion['total_cases'] = target_confusion['actual_class'].map(total_per_target)\n",
    "target_confusion['confusion_rate'] = target_confusion['cases'] / target_confusion['total_cases']\n",
    "target_confusion['rate_cases'] = target_confusion['cases'].astype(str) + ' / ' + target_confusion['total_cases'].astype(str)\n",
    "\n",
    "# Sort by number of cases (descending)\n",
    "target_confusion = target_confusion.sort_values('cases', ascending=False)\n",
    "\n",
    "# Display\n",
    "display(\n",
    "    target_confusion[['target_species', 'confusion_rate', 'rate_cases', 'mean_confidence']]\n",
    "    .rename(columns={\n",
    "        'target_species': 'Target Species',\n",
    "        'confusion_rate': 'Confusion Rate',\n",
    "        'rate_cases': 'Cases/All',\n",
    "        'mean_confidence': 'Confidence'\n",
    "    })\n",
    "    .head(10)\n",
    "    .style.format({'Confusion Rate': '{:.2%}', 'Confidence': '{:.3f}'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
