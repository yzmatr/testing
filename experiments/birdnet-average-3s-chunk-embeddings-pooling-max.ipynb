{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a259914-ee30-4e9b-a812-e393adb11ced",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Experiment & Config Setup\n",
    "\n",
    "Before starting any experiment, you should define the `EXPERIMENT_NAME` (ideally, matching the name of the notebook) and the baseline configuration for all the classes that will be used across the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#-------------------------------------------------------------------------------\n",
    "# REQUIRED PACKAGES\n",
    "#-------------------------------------------------------------------------------\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from databricks.sdk.runtime import *\n",
    "notebook_path =  '/Workspace/' + os.path.dirname(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get())\n",
    "os.chdir(notebook_path)\n",
    "os.chdir('..')\n",
    "sys.path.append(\"../..\")\n",
    "from mlops.utils.environment_setup import start_experiment\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# CONFIGURABLE OPTIONS\n",
    "#-------------------------------------------------------------------------------\n",
    "EXPERIMENT_NAME = \"birdnet-average-3s-chunk-embeddings-pooling-max\"     # Experiment name (should match notebook name)\n",
    "SCHEMA = \"frogid_ml\"\n",
    "MODEL_NAME = \"birdnet-average-3s-chunk-embeddings-pooling-max\"\n",
    "CURRENT_USER=\"hannah.weng@matrgroup.com\"                   # Author email address on databricks             \n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# SYSTEM SETUP FOR EXPERIMENT\n",
    "#-------------------------------------------------------------------------------\n",
    "IS_DATABRICKS = \"DATABRICKS_RUNTIME_VERSION\" in os.environ\n",
    "ROOT_DIR = Path(os.getcwd()).parent\n",
    "sys.path.insert(0, str(ROOT_DIR))\n",
    "experiment = start_experiment(\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    root_dir=ROOT_DIR, \n",
    "    is_databricks=IS_DATABRICKS, \n",
    "    current_user=CURRENT_USER\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e943c88-fde3-49eb-9baa-6bf3b564d5c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Run Setup\n",
    "\n",
    "In the following section you can run a version of this experiment by defining a run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# START MLFLOW RUN\n",
    "################################################################################\n",
    "# This an be a new run (run_id = None) or an existing run that you want to\n",
    "# reload, by specifying the run_id\n",
    "################################################################################\n",
    "from utils.environment_setup import start_mlflow_run\n",
    "from utils.pipeline import generate_pipeline_config, instantiate_pipeline\n",
    "\n",
    "run_name, run_id = start_mlflow_run(run_id = \"c1bfbab4aa2b410d96c1d61e9f0fca8b\")\n",
    "config = generate_pipeline_config(\n",
    "    experiment, \n",
    "    run_id, overrides={\n",
    "    \"evaluation_config.pooling_strategy\": \"mean\",\n",
    "    \"modelling_data_strategy.other_species_sampling_strategy\": \"stratify\"\n",
    "    }, \n",
    "    force_save=True)\n",
    "pipeline = instantiate_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# LOAD CLEAN DATA\n",
    "################################################################################\n",
    "# The anchoring function determines how you select the class_label_single\n",
    "# from a list of species in multi-species settings. Below we use the\n",
    "# most-frequent-target strategy, which means that if there are multiple species\n",
    "# the single class label will be the species that is most frequently represented\n",
    "# among the list of class labels.\n",
    "################################################################################\n",
    "\n",
    "from feature_engineering.registry_anchoring_strategies import ANCHORING_STRATEGY_REGISTRY\n",
    "\n",
    "# Load the cleaned data and their classes\n",
    "df_data, class_labels_to_species_mapping = pipeline.data_selector.load_data(\n",
    "    label_anchor_fn=ANCHORING_STRATEGY_REGISTRY[\"most-frequent-target\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# MODELLING\n",
    "################################################################################\n",
    "# The following code snippet demonstrates how to produce a reproducible ML\n",
    "# model training pipeline. The process involves:\n",
    "# 1. Selecting the subset of data to use for modelling based on a strategy\n",
    "# 2. Downloading & Preprocessing the selected subset to create a feature df\n",
    "# 3. Training the model according to the initial experiment setup\n",
    "################################################################################\n",
    "from mlops.training.tf_model_registry import MODEL_REGISTRY\n",
    "\n",
    "# Step 1: Sample the modelling data using the data_sampler\n",
    "df_modelling = pipeline.data_sampler.sample_modelling_dataset(\n",
    "    df_data=df_data,\n",
    "    modelling_strategy=config.modelling_data_strategy,\n",
    ")\n",
    "\n",
    "# Step 2: Download the data and return the updated dataframe (in case of missing files)\n",
    "df_modelling = pipeline.data_downloader.download_files(df_modelling)\n",
    "\n",
    "# Step 3: Create embeddings for the data using the data_preprocessor\n",
    "df_modelling_features = pipeline.data_preprocessor.run(df_modelling)\n",
    "\n",
    "# Step 4: Train the model\n",
    "model = pipeline.model_trainer.train(df_modelling_features, model_fn=MODEL_REGISTRY['birdnet_mlp_multiclass'], name = f\"{SCHEMA}.{MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# EVALUATION: TEST DATA\n",
    "################################################################################\n",
    "# The following code snippet demonstrates how to do an evaluation of a model\n",
    "# 1. Select a sample you are interested in using the data_sampler\n",
    "# 2. Downloading & Preprocessing the selected subset to create a feature df\n",
    "# 3. Evaluating the model by pointing to the correct run_id\n",
    "################################################################################\n",
    "\n",
    "# Sample the data you are interested in\n",
    "df_sample = pipeline.data_sampler.sample_test_data(run_id=run_id, df=df_data)\n",
    "\n",
    "# Download any files required to evaluate this sample\n",
    "df_sample = pipeline.data_downloader.download_files(df_sample)\n",
    "\n",
    "# Create embeddings for the data using this sample\n",
    "df_sample_features = pipeline.data_preprocessor.run(df_sample)\n",
    "\n",
    "# Evaluate the results for the model stored inside the given run_id\n",
    "print(\"üîÑ Starting model evaluation...\")\n",
    "print(f\"üìä Evaluating {len(df_sample_features)} feature samples\")\n",
    "print(f\"üè∑Ô∏è Using class mapping with {len(class_labels_to_species_mapping)} classes\")\n",
    "\n",
    "try:\n",
    "    y_true, y_true_binarized, y_pred, y_probs, macro_results, per_species_results = pipeline.model_evaluator.evaluate(\n",
    "        run_id=run_id,\n",
    "        df_features=df_sample_features,\n",
    "        class_label_to_species_mapping=class_labels_to_species_mapping,\n",
    "        dir_name_to_store_results=\"single-species-max-1000\"\n",
    "    )\n",
    "    print(\"‚úÖ Evaluation completed successfully!\")\n",
    "    print(f\"üìà Macro results: {macro_results}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Evaluation failed with error: {str(e)}\")\n",
    "    print(f\"üîç Error type: {type(e).__name__}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# EVALUATION: TEST DATA\n",
    "################################################################################\n",
    "# The following code snippet demonstrates how to do an evaluation of a model\n",
    "# 1. Select a sample you are interested in using the data_sampler\n",
    "# 2. Downloading & Preprocessing the selected subset to create a feature df\n",
    "# 3. Evaluating the model by pointing to the correct run_id\n",
    "################################################################################\n",
    "\n",
    "# Sample the data you are interested in\n",
    "df_sample = pipeline.data_sampler.sample_test_data(run_id=run_id, df=df_data)\n",
    "\n",
    "# Download any files required to evaluate this sample\n",
    "df_sample = pipeline.data_downloader.download_files(df_sample)\n",
    "\n",
    "# Create embeddings for the data using this sample\n",
    "df_sample_features = pipeline.data_preprocessor.run(df_sample)\n",
    "\n",
    "# Evaluate the results for the model stored inside the given run_id\n",
    "print(\"üîÑ Starting model evaluation...\")\n",
    "print(f\"üìä Evaluating {len(df_sample_features)} feature samples\")\n",
    "print(f\"üè∑Ô∏è Using class mapping with {len(class_labels_to_species_mapping)} classes\")\n",
    "\n",
    "try:\n",
    "    y_true, y_true_binarized, y_pred, y_probs, macro_results, per_species_results = pipeline.model_evaluator.evaluate(\n",
    "        run_id=run_id,\n",
    "        df_features=df_sample_features,\n",
    "        class_label_to_species_mapping=class_labels_to_species_mapping,\n",
    "        dir_name_to_store_results=\"single-species-mean-1000\"\n",
    "    )\n",
    "    print(\"‚úÖ Evaluation completed successfully!\")\n",
    "    print(f\"üìà Macro results: {macro_results}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Evaluation failed with error: {str(e)}\")\n",
    "    print(f\"üîç Error type: {type(e).__name__}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# EVALUATION: TEST DATA\n",
    "################################################################################\n",
    "# The following code snippet demonstrates how to do an evaluation of a model\n",
    "# 1. Select a sample you are interested in using the data_sampler\n",
    "# 2. Downloading & Preprocessing the selected subset to create a feature df\n",
    "# 3. Evaluating the model by pointing to the correct run_id\n",
    "################################################################################\n",
    "\n",
    "# Sample the data you are interested in\n",
    "df_sample = pipeline.data_sampler.sample_test_data(run_id=run_id, df=df_data)\n",
    "\n",
    "# Download any files required to evaluate this sample\n",
    "df_sample = pipeline.data_downloader.download_files(df_sample)\n",
    "\n",
    "# Create embeddings for the data using this sample\n",
    "df_sample_features = pipeline.data_preprocessor.run(df_sample)\n",
    "\n",
    "# Evaluate the results for the model stored inside the given run_id\n",
    "print(\"üîÑ Starting model evaluation...\")\n",
    "print(f\"üìä Evaluating {len(df_sample_features)} feature samples\")\n",
    "print(f\"üè∑Ô∏è Using class mapping with {len(class_labels_to_species_mapping)} classes\")\n",
    "\n",
    "try:\n",
    "    y_true, y_true_binarized, y_pred, y_probs, macro_results, per_species_results = pipeline.model_evaluator.evaluate(\n",
    "        run_id=run_id,\n",
    "        df_features=df_sample_features,\n",
    "        class_label_to_species_mapping=class_labels_to_species_mapping,\n",
    "        dir_name_to_store_results=\"single-species-topk-1000\"\n",
    "    )\n",
    "    print(\"‚úÖ Evaluation completed successfully!\")\n",
    "    print(f\"üìà Macro results: {macro_results}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Evaluation failed with error: {str(e)}\")\n",
    "    print(f\"üîç Error type: {type(e).__name__}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# EVALUATION: TEST DATA\n",
    "################################################################################\n",
    "# The following code snippet demonstrates how to do an evaluation of a model\n",
    "# 1. Select a sample you are interested in using the data_sampler\n",
    "# 2. Downloading & Preprocessing the selected subset to create a feature df\n",
    "# 3. Evaluating the model by pointing to the correct run_id\n",
    "################################################################################\n",
    "\n",
    "# Sample the data you are interested in\n",
    "df_sample = pipeline.data_sampler.sample_test_data(run_id=run_id, df=df_data)\n",
    "\n",
    "# Download any files required to evaluate this sample\n",
    "df_sample = pipeline.data_downloader.download_files(df_sample)\n",
    "\n",
    "# Create embeddings for the data using this sample\n",
    "df_sample_features = pipeline.data_preprocessor.run(df_sample)\n",
    "\n",
    "# Evaluate the results for the model stored inside the given run_id\n",
    "print(\"üîÑ Starting model evaluation...\")\n",
    "print(f\"üìä Evaluating {len(df_sample_features)} feature samples\")\n",
    "print(f\"üè∑Ô∏è Using class mapping with {len(class_labels_to_species_mapping)} classes\")\n",
    "\n",
    "try:\n",
    "    y_true, y_true_binarized, y_pred, y_probs, macro_results, per_species_results = pipeline.model_evaluator.evaluate(\n",
    "        run_id=run_id,\n",
    "        df_features=df_sample_features,\n",
    "        class_label_to_species_mapping=class_labels_to_species_mapping,\n",
    "        dir_name_to_store_results=\"single-species-softmax-1000\"\n",
    "    )\n",
    "    print(\"‚úÖ Evaluation completed successfully!\")\n",
    "    print(f\"üìà Macro results: {macro_results}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Evaluation failed with error: {str(e)}\")\n",
    "    print(f\"üîç Error type: {type(e).__name__}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# HELPFUL UTILITIES\n",
    "################################################################################\n",
    "# This section describes how you might choose to use some helpful utilities\n",
    "################################################################################\n",
    "from utils.cache_utils import clear_cache\n",
    "from dataclasses import replace\n",
    "\n",
    "# You can clear local directories by specifying what ids you want to keep\n",
    "deleted_files_list = clear_cache(\n",
    "    directory=experiment[\"audio_files_path\"],\n",
    "    keep_ids=[],\n",
    "    keep_extensions={\"wav\"}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
