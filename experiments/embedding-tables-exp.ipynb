{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f5ca809-de94-4a9b-afdf-77bb91193812",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Experiment & Config Setup\n",
    "\n",
    "Before starting any experiment, you should define the `EXPERIMENT_NAME` (ideally, matching the name of the notebook) and the baseline configuration for all the classes that will be used across the experiment.\n",
    "\n",
    "This notebook goes over how to usetilise the stored embeddings, add new embeddings to the database, and how to work with both stored embeddings and ones that also need to be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ead9dab7-8cd9-4fba-8ed5-1dcb52392e5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "#-------------------------------------------------------------------------------\n",
    "# REQUIRED PACKAGES\n",
    "#-------------------------------------------------------------------------------\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from mlops.utils.environment_setup import start_experiment\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# CONFIGURABLE OPTIONS\n",
    "#-------------------------------------------------------------------------------\n",
    "EXPERIMENT_NAME = \"testing-embeddings\"     # Experiment name (should match notebook name)\n",
    "CURRENT_USER=\"elise.hampton@matrgroup.com\"                   # Author email address on databricks             \n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# SYSTEM SETUP FOR EXPERIMENT\n",
    "#-------------------------------------------------------------------------------\n",
    "IS_DATABRICKS = \"DATABRICKS_RUNTIME_VERSION\" in os.environ\n",
    "ROOT_DIR = Path(os.getcwd()).parent\n",
    "sys.path.insert(0, str(ROOT_DIR))\n",
    "experiment = start_experiment(\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    root_dir=ROOT_DIR, \n",
    "    is_databricks=IS_DATABRICKS, \n",
    "    current_user=CURRENT_USER\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc818d32-454b-46d9-bac6-d200d4c9c7dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Run Setup\n",
    "\n",
    "In the following section you can run a version of this experiment by defining a run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b23edd1-51db-4bc4-a473-005cee779fc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# START MLFLOW RUN\n",
    "################################################################################\n",
    "# This an be a new run (run_id = None) or an existing run that you want to\n",
    "# reload, by specifying the run_id\n",
    "################################################################################\n",
    "from mlops.utils.environment_setup import start_mlflow_run\n",
    "from mlops.utils.pipeline import generate_pipeline_config, instantiate_pipeline\n",
    "\n",
    "run_name, run_id = start_mlflow_run(run_id=None)\n",
    "config = generate_pipeline_config(\n",
    "    experiment,\n",
    "    run_id,\n",
    "    overrides={\n",
    "    #    \"selector_config.target_species_definition.base_target_species\": [\n",
    "    #    \"Litoria verreauxii\",\n",
    "    #    \"Litoria ewingii\"\n",
    "    #]\n",
    "    #,\n",
    "    \"modelling_data_strategy.other_species_sampling_strategy\": \"stratify\"\n",
    "    #these next two parameters need to be changed in sync\n",
    "     ,\"preprocessor_config.birdnet_extractor_config.output_mode\" : \"stack\"\n",
    "     ,\"preprocessor_config.embeddings_databricks_config.embedding_strategy\":\"stack\"\n",
    "\n",
    "    ,\"modelling_data_strategy.target_species_max_samples_per_class\" : 5\n",
    "\n",
    "    #specific to the storing and retrieving of data\n",
    "    ,\"preprocessor_config.orchestrator_config.embeddings_save_format\" : \"dbx-table\"\n",
    "    ,\"preprocessor_config.embeddings_databricks_config.table\": \"aus_museum_dbx_dev.frogid_ml.dev_embed_table\" \n",
    "    #these two parameters are specific to the embeddings created trhough BirdNet, if you switch from BirdNet these need to be updated\n",
    "    ,\"preprocessor_config.embeddings_databricks_config.embeddings\": \"BirdNet\" \n",
    "    ,\"preprocessor_config.embeddings_databricks_config.window_duration\":3.0\n",
    "    #this parameter is currently 0.0 but when sliding windows are brought in there will be another parameter than will need to be in sync with this one\n",
    "    ,\"preprocessor_config.embeddings_databricks_config.overlap_duration\": 0.0\n",
    "    \n",
    "},\n",
    "    force_save=True)\n",
    "\n",
    "pipeline = instantiate_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b631199-9c5d-4402-89d8-afa652475fc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# LOAD CLEAN DATA\n",
    "################################################################################\n",
    "# The anchoring function determines how you select the class_label_single\n",
    "# from a list of species in multi-species settings. Below we use the\n",
    "# most-frequent-target strategy, which means that if there are multiple species\n",
    "# the single class label will be the species that is most frequently represented\n",
    "# among the list of class labels.\n",
    "################################################################################\n",
    "\n",
    "from mlops.feature_engineering.registry_anchoring_strategies import ANCHORING_STRATEGY_REGISTRY\n",
    "\n",
    "# Load the cleaned data and their classes\n",
    "df_data, class_labels_to_species_mapping = pipeline.data_selector.load_data(\n",
    "    label_anchor_fn=ANCHORING_STRATEGY_REGISTRY[\"most-frequent-target\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccd08dce-13bd-451d-b180-8baf76ca3abd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# MODELLING\n",
    "################################################################################\n",
    "# The following code snippet demonstrates how to produce a reproducible ML\n",
    "# model training pipeline. The process involves:\n",
    "# 1. Selecting the subset of data to use for modelling based on a strategy\n",
    "# 2. Downloading & Preprocessing the selected subset to create a feature df\n",
    "# 3. Training the model according to the initial experiment setup\n",
    "################################################################################\n",
    "from mlops.training.tf_model_registry import MODEL_REGISTRY\n",
    "\n",
    "# Step 1: Sample the modelling data using the data_sampler\n",
    "df_modelling = pipeline.data_sampler.sample_modelling_dataset(\n",
    "    df_data=df_data,\n",
    "    modelling_strategy=config.modelling_data_strategy,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8423055d-2b21-4ea9-a6a1-117894d619be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Step 2. -identify what data is already stored in the embeddings table and what is not\n",
    "df_modelling_comp, df_modelling = pipeline.data_databricks.data_embeddings_search(df_modelling)\n",
    "\n",
    "#Step 3. - download the data that is there and/or create the data we need (and store it for next time)\n",
    "\n",
    "#### Logic for both retrieveing and producing embeddings where needed ####\n",
    "#Create two empty dataframes\n",
    "# Define the columns and their desired data types\n",
    "columns_with_dtypes = {\n",
    "    'id': int,\n",
    "    'chunk_index': int,\n",
    "    'features': 'O',\n",
    "    'class_label': float,\n",
    "    'species_name': str\n",
    "}\n",
    "df_modelling_embed = pd.DataFrame({col: pd.Series(dtype=dtype) for col, dtype in columns_with_dtypes.items()})\n",
    "df_modelling_features = pd.DataFrame({col: pd.Series(dtype=dtype) for col, dtype in columns_with_dtypes.items()})\n",
    "\n",
    "if df_modelling_comp.empty: \n",
    "    print(\"No data in database - need to download and produce embeddings for all.\")\n",
    "else:\n",
    "    #Step3.1. - if there is data in the database\n",
    "    #go and get the embeddings from the database\n",
    "    df_modelling_embed = pipeline.data_databricks.data_embeddings_retrieve(df_modelling_comp)\n",
    "    \n",
    "if df_modelling.empty == False: #go and get the missing data and add to the embeddings database\n",
    "    # Step 3.2: Download the data and return the updated dataframe (in case of missing files)\n",
    "    df_modelling = pipeline.data_downloader.download_files(df_modelling)\n",
    "    # Step 3.3: Create embeddings for the data using the data_preprocessor\n",
    "    df_modelling_features = pipeline.data_preprocessor.run(df_modelling)\n",
    "\n",
    "#Step 3.4: Combine the data together in cases where the data is being retrieved two ways\n",
    "df_modelling_features = pd.concat([df_modelling_features, df_modelling_embed], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc01f5c7-89ea-4f2c-a183-3c27d0830459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Train the model\n",
    "model = pipeline.model_trainer.train(df_modelling_features, model_fn=MODEL_REGISTRY['birdnet_mlp_multiclass'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5edb4f3e-1452-44fa-8f7a-8283fce18c18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Before you continue\n",
    "There are two options going forward: \n",
    "1. All the data you need is being stored and you only need to call \n",
    "> pipeline.data_databricks.data_embeddings_retrieve(df_modelling) \n",
    "2. You don't know what data is avaliable and you will need to use the same logic as used above to work out what data you want is missing and to add it into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aab8dfef-77f3-4144-a730-db5174a4fd49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# EVALUATION: TEST DATA\n",
    "################################################################################\n",
    "# The following code snippet demonstrates how to do an evaluation of a model\n",
    "# 1. Select a sample you are interested in using the data_sampler\n",
    "# 2. Downloading & Preprocessing the selected subset to create a feature df\n",
    "# 3. Evaluating the model by pointing to the correct run_id\n",
    "################################################################################\n",
    "\n",
    "# Sample the data you are interested in\n",
    "df_sample = pipeline.data_sampler.sample_test_data(run_id=run_id, df=df_data)\n",
    "\n",
    "\n",
    "# Download any files required to evaluate this sample\n",
    "df_sample = pipeline.data_downloader.download_files(df_sample)\n",
    "# Create embeddings for the data using this sample\n",
    "df_sample_features = pipeline.data_preprocessor.run(df_sample)\n",
    "\n",
    "\n",
    "# Evaluate the results for the model stored inside the given run_id\n",
    "y_true, y_true_binarized, y_pred, y_probs, macro_results, per_species_results = pipeline.model_evaluator.evaluate(\n",
    "    run_id=run_id,\n",
    "    df_features=df_sample_features,\n",
    "    class_label_to_species_mapping=class_labels_to_species_mapping,\n",
    "    dir_name_to_store_results=\"single-species-max-1000\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d0fdf17-cac9-480b-a03d-c8a4c02fc631",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# EVALUATION: HOLDOUT DATA\n",
    "################################################################################\n",
    "# The following code snippet demonstrates how to do an evaluation of a model\n",
    "# 1. Select a sample you are interested in using the data_sampler\n",
    "# 2. Downloading & Preprocessing the selected subset to create a feature df\n",
    "# 3. Evaluating the model by pointing to the correct run_id\n",
    "################################################################################\n",
    "from mlops.feature_engineering.registry_filtering_strategies import FILTERING_STRATEGY_REGISTRY\n",
    "\n",
    "# Sample the data you are interested in\n",
    "df_sample = pipeline.data_sampler.sample_hold_out_data(\n",
    "    run_id=run_id,\n",
    "    df_cleaned=df_data,\n",
    "    filtering_strategy_fn=FILTERING_STRATEGY_REGISTRY['single-species-only'],\n",
    "    max_samples_per_class=100,\n",
    ")\n",
    "\n",
    "# Download any files required to evaluate this sample\n",
    "df_sample = pipeline.data_downloader.download_files(df_sample)\n",
    "\n",
    "# Create embeddings for the data using this sample\n",
    "df_sample_features = pipeline.data_preprocessor.run(df_sample)\n",
    "\n",
    "# Evaluate the results for the model stored inside the given run_id\n",
    "y_true, y_true_binarized, y_pred, y_probs, macro_results, per_species_results = pipeline.model_evaluator.evaluate(\n",
    "    run_id=run_id,\n",
    "    df_features=df_sample_features,\n",
    "    class_label_to_species_mapping=class_labels_to_species_mapping,\n",
    "    dir_name_to_store_results=\"holdout-data-max-100\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "embedding-tables-exp",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
